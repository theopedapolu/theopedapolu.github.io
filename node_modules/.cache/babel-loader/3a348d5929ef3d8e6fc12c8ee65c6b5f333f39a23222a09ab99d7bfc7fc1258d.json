{"ast":null,"code":"import React from\"react\";import\"./cs180_styles.css\";import{jsx as _jsx}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";import{Fragment as _Fragment}from\"react/jsx-runtime\";function NERF(){return/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"h1\",{children:\"Neural Radiance Fields\"}),/*#__PURE__*/_jsx(\"h2\",{style:{textAlign:\"center\"},children:\"Theophilus Pedapolu\"}),/*#__PURE__*/_jsx(\"h2\",{children:\"Background\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"In this project, we explore how a neural radiance field (NeRF) can be used to represent 3D space. A NeRF is a function F: \",\"{\",\"x,y,z,\\u03B8,\\u03C6\",\"}\",\"\\u2192\",\"{\",\"r,g,b,\\u03C3\",\"}\",\" that takes in world coordinates \",/*#__PURE__*/_jsx(\"i\",{children:\"x,y,z\"}),\" and a viewing direction \\u03B8,\\u03C6 and returns a color value \",/*#__PURE__*/_jsx(\"i\",{children:\"r,g,b\"}),\" and density \",/*#__PURE__*/_jsx(\"i\",{children:\"\\u03C3\"}),\" \",\"that is used to create a 2D projection of the world from that perspective. This function is found by training a neural network. In part 1, we consider a simple case of a NeRF for a 2D image and, in part 2, we train a deeper neural network to represent a 3D space\"]}),/*#__PURE__*/_jsx(\"h2\",{children:\"Part 1: Fitting a Neural Radiance Field to a 2D Image\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"To understand radiance fields, we consider a simple NeRF\",\" \",/*#__PURE__*/_jsxs(\"i\",{children:[\"F: \",\"{\",\"u,v\",\"}\",\" \\u2192 \",\"{\",\"r,g,b\",\"}\"]}),\" \",\"that maps 2D pixel coordinates to a color value. We train a shallow neural network to learn this function on an image and optimize it to reconstruct the image. We use the MLP architecture shown below, with a sigmoid activation at the end to ensure the RGB values lie in the normalized range (0,1). Furthermore, to enable local pixels to have sufficient variability in their RGB values, we use positional encoding. A series of sinusoidal functions is applied to the 2D input coordinates to expand its dimensionality to 42. We also normalized the pixels values and pixel coordinates before training the network. In training the network, for each iteration, we sample a batch of 10,000 random pixels from the image, using the pixel coordinates as the input and known RGB values for each pixel as the target. Our loss function is the peak signal-to-noise ratio (PSNR).\",\" \"]}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/part1_MLP.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/part1_MLP.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"MLP Architecture for Simple NeRF\"})]})})}),/*#__PURE__*/_jsx(\"p\",{children:\"The training PSNR and predicted images across intermediate iterations are shown below. The hyperparameters used were: batch_size = 10000, learning_rate = 1e-2, L = 10, number_of_layers = 4, channel_size = 256. Overall, the model reconstructs the image pretty well but it is still a little blurry compared to the original image. Perhaps more iterations are needed to allow the model to learn higher dimensional features\"}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox_psnr.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox_psnr.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox_ims.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox_ims.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Predicted Fox Images during Training\"})]})})}),/*#__PURE__*/_jsx(\"p\",{children:\"We also changed the channel size hyperparameter from 256 to 420 to see how the performance is affected. This caused the model to perform worse and get a lower PSNR, perhaps because more iterations are needed to correctly update all the weights. The final fox image looks okay but is blurrier than the predicted image with the original hyperparameters\"}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox2_psnr.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox2_psnr.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox2_ims.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox2_ims.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Predicted Fox Images during Training\"})]})})}),/*#__PURE__*/_jsx(\"p\",{children:\"Additionally, we changed the hyperparameter for the length of the positional encoding, L from 10 to 3. This caused the performance to be signifcantly worse, as seen in the predicted images across iterations. The model is much slower in converging to a high PSNR so the final result after 2000 iterations is much blurrier. This is probably because, with a lower dimsional positional encoding, it takes longer for the model to discrimiate colors for pixels that are spatially close\"}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox3_psnr.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox3_psnr.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/fox3_ims.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/fox3_ims.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Predicted Fox Images during Training\"})]})})}),/*#__PURE__*/_jsx(\"p\",{children:\"Finally, we optimized this model to reconstruct an image of El Capitan in Yosemite National Park. The hyperparameters used were: batch_size = 30000, channel_size = 300, L = 15, number_of_layer = 4, learning_rate = 1e-2. Overall, the model fit the image well and reconstructs most of it with a good PSNR.\"}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/capitan_psnr.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/capitan_psnr.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/capitan_ims.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/capitan_ims.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Predicted Fox Images during Training\"})]})})}),/*#__PURE__*/_jsx(\"h2\",{children:\"Pat 2: Fitting a Neural Radiance Field to Multi-View Images\"}),/*#__PURE__*/_jsx(\"p\",{children:\"Now that we understand the basics of NeRFs, we train a deeper NeRF to represent the 3D space of a lego truck. There are a number of preprocessing and postprocessing steps we implement to train this more complex model\"}),/*#__PURE__*/_jsx(\"h3\",{children:\"Create Rays from Cameras\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"First, we write functions to convert between pixel, camera, and world coordinates and create rays from a pixel. To convert from camera to world coordinates, we create a function that takes in a camera-to-world (c2w) matrix and a coordinates tensor of shape (batch_size, 3). A column of ones is appended to the coordinates tensor (to allow affine transformations) then it is tranposed and multiplied with the c2w matrix to generate the world coordinates. To convert from pixel to camera coordinates, a similar process is employed, except we multiply by the inverse of the intrinsic matrix\",\" \",/*#__PURE__*/_jsx(\"b\",{children:\"K\"}),\" and a depth s to get 3D coordinates in the camera system. Finally, we write a function to create rays from 2D pixel coordinates. The origin of the rays is found by converting the camera coordinates [0,0,0] to world coordinates (i.e. we multiply the c2w matrix by the vector [0,0,0,1]). And the ray direction for each pixel \",/*#__PURE__*/_jsx(\"i\",{children:\"(u,v)\"}),\" is found by first converting the pixel to camera coordinates, then world coordinates to get\",\" \",/*#__PURE__*/_jsxs(\"b\",{children:[\"X\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"})]}),\" \",\"=\",\" \",/*#__PURE__*/_jsxs(\"i\",{children:[\"(x\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"}),\" y\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"}),\" z\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"}),\")\"]}),\". We then use the formula\",\" \",/*#__PURE__*/_jsxs(\"b\",{children:[\"r\",/*#__PURE__*/_jsx(\"sub\",{children:\"d\"})]}),\" \",\"= (X\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"}),\"-r\",/*#__PURE__*/_jsx(\"sub\",{children:\"0\"}),\") / ||X\",/*#__PURE__*/_jsx(\"sub\",{children:\"w\"}),\"-r\",/*#__PURE__*/_jsx(\"sub\",{children:\"0\"}),\"||\",/*#__PURE__*/_jsx(\"sub\",{children:\"2\"}),\" where r\",/*#__PURE__*/_jsx(\"sub\",{children:\"0\"}),\" is the ray origin to get the normalized ray direction. All of these functions support batched coordinates by taking in a tensor of shape (batch_size,3), i.e. the coordinates are laid out in rows\"]}),/*#__PURE__*/_jsx(\"h3\",{children:\"Sampling\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"Next we implement random sampling to get rays from different perspectives. First, we write a function \",/*#__PURE__*/_jsx(\"code\",{children:\"sample_rays\"}),\" which uses a set of images and corresponding c2w matrices and returns a sampling of N ray origins, ray directions, and RGB values from the pixels across all images. We do this by sampling M random images, then sampling N//M random rays from each image, keeping track of what the actual RGB values for each ray are for training purposes. We found that the values N = 10000, M = 50 worked pretty well in producing a low PSNR. We also wrote a function\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"sample_along_rays\"}),\" which takes in a set of ray origins and ray directions and discretizes each ray by sampling evenly at points along the ray. Namely, we sample 64 points evenly from 2.0 to 6.0 (i.e.\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"t = np.linspace(2.0,6.0,64)\"}),\") along the ray to get the 3D coordinates\",\" \",/*#__PURE__*/_jsxs(\"code\",{children:[\"X = R\",/*#__PURE__*/_jsx(\"sub\",{children:\"o\"}),\" + R\",/*#__PURE__*/_jsx(\"sub\",{children:\"d\"}),\" * t\"]}),\" \",\"for each ray. Finally, during training only, we add some small perturbation (in the range [0,0.1]) to the points \",/*#__PURE__*/_jsx(\"code\",{children:\"t\"}),\" along which we sample to ensure every location along the ray is touched upon in training. In the end, we go from R\",/*#__PURE__*/_jsx(\"sub\",{children:\"o\"}),\" and R\",/*#__PURE__*/_jsx(\"sub\",{children:\"d\"}),\" tensors of shape\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"(batch_size,3)\"}),\" to a points tensor of shape\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"(batch_size,64,3)\"})]}),/*#__PURE__*/_jsx(\"h3\",{children:\"Dataloader\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"We use all the functions we made in the previous 2 parts to implement a custom dataset that takes in a set of images, corresponding c2w matrices, an intrinsic matrix \",/*#__PURE__*/_jsx(\"b\",{children:\"K\"}),\", a parameter M for how many images to sample in\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"sample_rays\"}),\", and a flag \",/*#__PURE__*/_jsx(\"code\",{children:\"perturb\"}),\" to indicate wheter perturbation should be added. The dataset returns a tensor of shape\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"(10000, 64, 3)\"}),\" for each iteration, i.e. 10,000 rays and 64 sample points along each ray as well as a rays direction tensor\",\" \",/*#__PURE__*/_jsxs(\"b\",{children:[\"R\",/*#__PURE__*/_jsx(\"sub\",{children:\"d\"})]}),\" \",\"and a pixels tensor, both of shape \",/*#__PURE__*/_jsx(\"code\",{children:\"(10000,3)\"}),\" for feeding through the model. A visualization of how this sampling looks like for the set of multi-view lego truck images is shown below.\",\" \"]}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/viser.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/viser.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"h3\",{children:\"NeRF Architecture\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"Now that all the preprocessing steps have been implemented, we create a neural network architecture that learns a mapping from 3D world coordinates and viewing direction to RGB values and density. We use the architecture shown below, which takes in two tensors, one containing world coordinates and the other ray directions (or viewing directions). These inputs are fed through many MLP layers to get a tensors of densities and RBG values [both of shape \",/*#__PURE__*/_jsx(\"code\",{children:\"(batch_size,64,3)\"}),\"] at the end. Since we are representing a 3D space, which is much more complex than a 2D space, we need a deeper network to learn the function. Moreover, the skip connections serve to alleviate the vanishing gradient problem so the network doesn't \\\"forget\\\" about the earlier layers and inputs. For positional encoding, we used L=10 for the \",/*#__PURE__*/_jsx(\"b\",{children:\"x\"}),\" tensor of world coordinates and L=4 for the\",\" \",/*#__PURE__*/_jsxs(\"b\",{children:[\"r\",/*#__PURE__*/_jsx(\"sub\",{children:\"d\"})]}),\" \",\"tensor of ray directions.\",\" \"]}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/part2_mlp.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/part2_mlp.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"NeRF Architecture for 3D Space\"})]})})}),/*#__PURE__*/_jsx(\"h3\",{children:\"Volume Rendering\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"After getting the output tensors from the model, we need to translate the density and RGB information for each sample to a single RGB value for each ray. We do this via the volume rendering equation shown below, which sums up the RGB values for each sample along the ray according to their corresponding density to get a single RBG value for the ray at the end. We vectorize this equation using \",/*#__PURE__*/_jsx(\"code\",{children:\"torch.cumsum\"}),\" and other\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"torch\"}),\" operatations and make it a part of the NeRF model. So our model actually returns a tensor of shape \",/*#__PURE__*/_jsx(\"code\",{children:\"(batch_size,3)\"}),\" \",\"containing the predicted RGB values for each ray\"]}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/volrend_eq.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/volrend_eq.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Volume Rendering Equation\"})]})})}),/*#__PURE__*/_jsx(\"h3\",{children:\"Results\"}),/*#__PURE__*/_jsx(\"p\",{children:\"The PSNR curve on the validation set for the first 1000 training iterations and the predicted images for the first validation image on the iterations 1,50,100,200,500,1000 are shown below\"}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",children:/*#__PURE__*/_jsx(\"div\",{className:\"gallery\",children:/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/lego_psnr.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/lego_psnr.jpg\",alt:\"cameraman_dx\"})})})})}),/*#__PURE__*/_jsx(\"div\",{className:\"container\",children:/*#__PURE__*/_jsx(\"div\",{className:\"responsive\",style:{width:\"100%\"},children:/*#__PURE__*/_jsxs(\"div\",{className:\"gallery\",children:[/*#__PURE__*/_jsx(\"a\",{target:\"_blank\",href:\"images/nerfs/lego_ims.jpg\",children:/*#__PURE__*/_jsx(\"img\",{src:\"images/nerfs/lego_ims.jpg\",alt:\"cameraman_dx\"})}),/*#__PURE__*/_jsx(\"div\",{className:\"desc\",children:\"Predicted Lego Validation Images During Training\"})]})})}),/*#__PURE__*/_jsx(\"p\",{children:\"Running our trained model on the c2ws_test extrinsics provided allows us to produce 60 frames of the lego truck image rotating counterclockwise. We merge these frames to render a video\"}),/*#__PURE__*/_jsx(\"video\",{width:640,height:320,autoPlay:\"\",muted:\"\",loop:\"\",children:/*#__PURE__*/_jsx(\"source\",{src:\"images/nerfs/lego.mp4\",type:\"video/mp4\"})}),/*#__PURE__*/_jsx(\"h3\",{children:\"Bells & Whistles\"}),/*#__PURE__*/_jsx(\"h4\",{children:\"Depth-Rendered Video\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"We also rendered a depths-map video by replacing the per-point color vector in the volume rendering equation with a scalar representing the depth of the point along the ray. Namely, the depths along the ray are represented by\",\" \",/*#__PURE__*/_jsx(\"code\",{children:\"linspace(1,0,64)\"}),\", 64 evenly spaced depths from 0 to 1, one for each point. This gives the depth-map video below:\"]}),/*#__PURE__*/_jsx(\"video\",{width:640,height:320,autoPlay:\"\",muted:\"\",loop:\"\",children:/*#__PURE__*/_jsx(\"source\",{src:\"images/nerfs/lego_depth.mp4\",type:\"video/mp4\"})}),/*#__PURE__*/_jsx(\"h4\",{children:\"Color-Rendered Video\"}),/*#__PURE__*/_jsxs(\"p\",{children:[\"Finally, we rendered a video of the lego truck with a different background color by calculating\",\" \",/*#__PURE__*/_jsxs(\"code\",{children:[\"1 - sum(T\",/*#__PURE__*/_jsx(\"sub\",{children:\"i\"}),\"*(1-exp(\\u03C3\",/*#__PURE__*/_jsx(\"sub\",{children:\"i\"}),\"\\u03B4\",/*#__PURE__*/_jsx(\"sub\",{children:\"i\"}),\")))*color\"]}),\" \",\"for each ray, where color is a tensor of shape [3] containing the RGB value for the background color. We then add this to the tensor of colors that were rendered normally to get an image with the background color changed. Conceptually, this works because it is tantamount to adding the expected value that the ray is a background ray with color \",/*#__PURE__*/_jsx(\"code\",{children:\"color\"}),\", which is just the probability that the ray doesn't terminate at any of the points multiplied by \",/*#__PURE__*/_jsx(\"code\",{children:\"color\"}),\". After making this change to the volume rendering equation, we can change the background color of the lego image as shown below:\"]}),/*#__PURE__*/_jsx(\"video\",{width:640,height:320,autoPlay:\"\",muted:\"\",loop:\"\",children:/*#__PURE__*/_jsx(\"source\",{src:\"images/nerfs/lego_color.mp4\",type:\"video/mp4\"})})]});}export default NERF;","map":{"version":3,"names":["React","jsx","_jsx","jsxs","_jsxs","Fragment","_Fragment","NERF","children","style","textAlign","className","target","href","src","alt","width","height","autoPlay","muted","loop","type"],"sources":["/Users/theopedapolu/theopedapolu.github.io/src/components/NERF.js"],"sourcesContent":["import React from \"react\";\nimport \"./cs180_styles.css\";\n\nfunction NERF() {\n    return (\n    <>\n    <h1>Neural Radiance Fields</h1>\n    <h2 style={{ textAlign: \"center\" }}>Theophilus Pedapolu</h2>\n    <h2>Background</h2>\n    <p>\n        In this project, we explore how a neural radiance field (NeRF) can be used\n        to represent 3D space. A NeRF is a function F: {\"{\"}x,y,z,θ,φ{\"}\"}→{\"{\"}\n        r,g,b,σ{\"}\"} that takes in world coordinates <i>x,y,z</i> and a viewing\n        direction θ,φ and returns a color value <i>r,g,b</i> and density <i>σ</i>{\" \"}\n        that is used to create a 2D projection of the world from that perspective.\n        This function is found by training a neural network. In part 1, we consider\n        a simple case of a NeRF for a 2D image and, in part 2, we train a deeper\n        neural network to represent a 3D space\n    </p>\n    <h2>Part 1: Fitting a Neural Radiance Field to a 2D Image</h2>\n    <p>\n        To understand radiance fields, we consider a simple NeRF{\" \"}\n        <i>\n        F: {\"{\"}u,v{\"}\"} → {\"{\"}r,g,b{\"}\"}\n        </i>{\" \"}\n        that maps 2D pixel coordinates to a color value. We train a shallow neural\n        network to learn this function on an image and optimize it to reconstruct\n        the image. We use the MLP architecture shown below, with a sigmoid\n        activation at the end to ensure the RGB values lie in the normalized range\n        (0,1). Furthermore, to enable local pixels to have sufficient variability in\n        their RGB values, we use positional encoding. A series of sinusoidal\n        functions is applied to the 2D input coordinates to expand its\n        dimensionality to 42. We also normalized the pixels values and pixel\n        coordinates before training the network. In training the network, for each\n        iteration, we sample a batch of 10,000 random pixels from the image, using\n        the pixel coordinates as the input and known RGB values for each pixel as\n        the target. Our loss function is the peak signal-to-noise ratio (PSNR).{\" \"}\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/part1_MLP.jpg\">\n            <img src=\"images/nerfs/part1_MLP.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">MLP Architecture for Simple NeRF</div>\n        </div>\n        </div>\n    </div>\n    <p>\n        The training PSNR and predicted images across intermediate iterations are\n        shown below. The hyperparameters used were: batch_size = 10000,\n        learning_rate = 1e-2, L = 10, number_of_layers = 4, channel_size = 256.\n        Overall, the model reconstructs the image pretty well but it is still a\n        little blurry compared to the original image. Perhaps more iterations are\n        needed to allow the model to learn higher dimensional features\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox_psnr.jpg\">\n            <img src=\"images/nerfs/fox_psnr.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox_ims.jpg\">\n            <img src=\"images/nerfs/fox_ims.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">Predicted Fox Images during Training</div>\n        </div>\n        </div>\n    </div>\n    <p>\n        We also changed the channel size hyperparameter from 256 to 420 to see how\n        the performance is affected. This caused the model to perform worse and get\n        a lower PSNR, perhaps because more iterations are needed to correctly update\n        all the weights. The final fox image looks okay but is blurrier than the\n        predicted image with the original hyperparameters\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox2_psnr.jpg\">\n            <img src=\"images/nerfs/fox2_psnr.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox2_ims.jpg\">\n            <img src=\"images/nerfs/fox2_ims.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">Predicted Fox Images during Training</div>\n        </div>\n        </div>\n    </div>\n    <p>\n        Additionally, we changed the hyperparameter for the length of the positional\n        encoding, L from 10 to 3. This caused the performance to be signifcantly\n        worse, as seen in the predicted images across iterations. The model is much\n        slower in converging to a high PSNR so the final result after 2000\n        iterations is much blurrier. This is probably because, with a lower\n        dimsional positional encoding, it takes longer for the model to discrimiate\n        colors for pixels that are spatially close\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox3_psnr.jpg\">\n            <img src=\"images/nerfs/fox3_psnr.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/fox3_ims.jpg\">\n            <img src=\"images/nerfs/fox3_ims.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">Predicted Fox Images during Training</div>\n        </div>\n        </div>\n    </div>\n    <p>\n        Finally, we optimized this model to reconstruct an image of El Capitan in\n        Yosemite National Park. The hyperparameters used were: batch_size = 30000,\n        channel_size = 300, L = 15, number_of_layer = 4, learning_rate = 1e-2.\n        Overall, the model fit the image well and reconstructs most of it with a\n        good PSNR.\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/capitan_psnr.jpg\">\n            <img src=\"images/nerfs/capitan_psnr.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/capitan_ims.jpg\">\n            <img src=\"images/nerfs/capitan_ims.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">Predicted Fox Images during Training</div>\n        </div>\n        </div>\n    </div>\n    <h2>Pat 2: Fitting a Neural Radiance Field to Multi-View Images</h2>\n    <p>\n        Now that we understand the basics of NeRFs, we train a deeper NeRF to\n        represent the 3D space of a lego truck. There are a number of preprocessing\n        and postprocessing steps we implement to train this more complex model\n    </p>\n    <h3>Create Rays from Cameras</h3>\n    <p>\n        First, we write functions to convert between pixel, camera, and world\n        coordinates and create rays from a pixel. To convert from camera to world\n        coordinates, we create a function that takes in a camera-to-world (c2w)\n        matrix and a coordinates tensor of shape (batch_size, 3). A column of ones\n        is appended to the coordinates tensor (to allow affine transformations) then\n        it is tranposed and multiplied with the c2w matrix to generate the world\n        coordinates. To convert from pixel to camera coordinates, a similar process\n        is employed, except we multiply by the inverse of the intrinsic matrix{\" \"}\n        <b>K</b> and a depth s to get 3D coordinates in the camera system. Finally,\n        we write a function to create rays from 2D pixel coordinates. The origin of\n        the rays is found by converting the camera coordinates [0,0,0] to world\n        coordinates (i.e. we multiply the c2w matrix by the vector [0,0,0,1]). And\n        the ray direction for each pixel <i>(u,v)</i> is found by first converting\n        the pixel to camera coordinates, then world coordinates to get{\" \"}\n        <b>\n        X<sub>w</sub>\n        </b>{\" \"}\n        ={\" \"}\n        <i>\n        (x<sub>w</sub> y<sub>w</sub> z<sub>w</sub>)\n        </i>\n        . We then use the formula{\" \"}\n        <b>\n        r<sub>d</sub>\n        </b>{\" \"}\n        = (X<sub>w</sub>-r<sub>0</sub>) / ||X<sub>w</sub>-r<sub>0</sub>||\n        <sub>2</sub> where r<sub>0</sub> is the ray origin to get the normalized ray\n        direction. All of these functions support batched coordinates by taking in a\n        tensor of shape (batch_size,3), i.e. the coordinates are laid out in rows\n    </p>\n    <h3>Sampling</h3>\n    <p>\n        Next we implement random sampling to get rays from different perspectives.\n        First, we write a function <code>sample_rays</code> which uses a set of\n        images and corresponding c2w matrices and returns a sampling of N ray\n        origins, ray directions, and RGB values from the pixels across all images.\n        We do this by sampling M random images, then sampling N//M random rays from\n        each image, keeping track of what the actual RGB values for each ray are for\n        training purposes. We found that the values N = 10000, M = 50 worked pretty\n        well in producing a low PSNR. We also wrote a function{\" \"}\n        <code>sample_along_rays</code> which takes in a set of ray origins and ray\n        directions and discretizes each ray by sampling evenly at points along the\n        ray. Namely, we sample 64 points evenly from 2.0 to 6.0 (i.e.{\" \"}\n        <code>t = np.linspace(2.0,6.0,64)</code>) along the ray to get the 3D\n        coordinates{\" \"}\n        <code>\n        X = R<sub>o</sub> + R<sub>d</sub> * t\n        </code>{\" \"}\n        for each ray. Finally, during training only, we add some small perturbation\n        (in the range [0,0.1]) to the points <code>t</code> along which we sample to\n        ensure every location along the ray is touched upon in training. In the end,\n        we go from R<sub>o</sub> and R<sub>d</sub> tensors of shape{\" \"}\n        <code>(batch_size,3)</code> to a points tensor of shape{\" \"}\n        <code>(batch_size,64,3)</code>\n    </p>\n    <h3>Dataloader</h3>\n    <p>\n        We use all the functions we made in the previous 2 parts to implement a\n        custom dataset that takes in a set of images, corresponding c2w matrices, an\n        intrinsic matrix <b>K</b>, a parameter M for how many images to sample in{\" \"}\n        <code>sample_rays</code>, and a flag <code>perturb</code> to indicate wheter\n        perturbation should be added. The dataset returns a tensor of shape{\" \"}\n        <code>(10000, 64, 3)</code> for each iteration, i.e. 10,000 rays and 64\n        sample points along each ray as well as a rays direction tensor{\" \"}\n        <b>\n        R<sub>d</sub>\n        </b>{\" \"}\n        and a pixels tensor, both of shape <code>(10000,3)</code> for feeding\n        through the model. A visualization of how this sampling looks like for the\n        set of multi-view lego truck images is shown below.{\" \"}\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/viser.jpg\">\n            <img src=\"images/nerfs/viser.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <h3>NeRF Architecture</h3>\n    <p>\n        Now that all the preprocessing steps have been implemented, we create a\n        neural network architecture that learns a mapping from 3D world coordinates\n        and viewing direction to RGB values and density. We use the architecture\n        shown below, which takes in two tensors, one containing world coordinates\n        and the other ray directions (or viewing directions). These inputs are fed\n        through many MLP layers to get a tensors of densities and RBG values [both\n        of shape <code>(batch_size,64,3)</code>] at the end. Since we are\n        representing a 3D space, which is much more complex than a 2D space, we need\n        a deeper network to learn the function. Moreover, the skip connections serve\n        to alleviate the vanishing gradient problem so the network doesn't \"forget\"\n        about the earlier layers and inputs. For positional encoding, we used L=10\n        for the <b>x</b> tensor of world coordinates and L=4 for the{\" \"}\n        <b>\n        r<sub>d</sub>\n        </b>{\" \"}\n        tensor of ray directions.{\" \"}\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/part2_mlp.jpg\">\n            <img src=\"images/nerfs/part2_mlp.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">NeRF Architecture for 3D Space</div>\n        </div>\n        </div>\n    </div>\n    <h3>Volume Rendering</h3>\n    <p>\n        After getting the output tensors from the model, we need to translate the\n        density and RGB information for each sample to a single RGB value for each\n        ray. We do this via the volume rendering equation shown below, which sums up\n        the RGB values for each sample along the ray according to their\n        corresponding density to get a single RBG value for the ray at the end. We\n        vectorize this equation using <code>torch.cumsum</code> and other{\" \"}\n        <code>torch</code> operatations and make it a part of the NeRF model. So our\n        model actually returns a tensor of shape <code>(batch_size,3)</code>{\" \"}\n        containing the predicted RGB values for each ray\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/volrend_eq.jpg\">\n            <img src=\"images/nerfs/volrend_eq.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">Volume Rendering Equation</div>\n        </div>\n        </div>\n    </div>\n    <h3>Results</h3>\n    <p>\n        The PSNR curve on the validation set for the first 1000 training iterations\n        and the predicted images for the first validation image on the iterations\n        1,50,100,200,500,1000 are shown below\n    </p>\n    <div className=\"container\">\n        <div className=\"responsive\">\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/lego_psnr.jpg\">\n            <img src=\"images/nerfs/lego_psnr.jpg\" alt=\"cameraman_dx\" />\n            </a>\n        </div>\n        </div>\n    </div>\n    <div className=\"container\">\n        <div className=\"responsive\" style={{ width: \"100%\" }}>\n        <div className=\"gallery\">\n            <a target=\"_blank\" href=\"images/nerfs/lego_ims.jpg\">\n            <img src=\"images/nerfs/lego_ims.jpg\" alt=\"cameraman_dx\" />\n            </a>\n            <div className=\"desc\">\n            Predicted Lego Validation Images During Training\n            </div>\n        </div>\n        </div>\n    </div>\n    <p>\n        Running our trained model on the c2ws_test extrinsics provided allows us to\n        produce 60 frames of the lego truck image rotating counterclockwise. We\n        merge these frames to render a video\n    </p>\n    <video width={640} height={320} autoPlay=\"\" muted=\"\" loop=\"\">\n        <source src=\"images/nerfs/lego.mp4\" type=\"video/mp4\" />\n    </video>\n    <h3>Bells &amp; Whistles</h3>\n    <h4>Depth-Rendered Video</h4>\n    <p>\n        We also rendered a depths-map video by replacing the per-point color vector\n        in the volume rendering equation with a scalar representing the depth of the\n        point along the ray. Namely, the depths along the ray are represented by{\" \"}\n        <code>linspace(1,0,64)</code>, 64 evenly spaced depths from 0 to 1, one for\n        each point. This gives the depth-map video below:\n    </p>\n    <video width={640} height={320} autoPlay=\"\" muted=\"\" loop=\"\">\n        <source src=\"images/nerfs/lego_depth.mp4\" type=\"video/mp4\" />\n    </video>\n    <h4>Color-Rendered Video</h4>\n    <p>\n        Finally, we rendered a video of the lego truck with a different background\n        color by calculating{\" \"}\n        <code>\n        1 - sum(T<sub>i</sub>*(1-exp(σ<sub>i</sub>δ<sub>i</sub>)))*color\n        </code>{\" \"}\n        for each ray, where color is a tensor of shape [3] containing the RGB value\n        for the background color. We then add this to the tensor of colors that were\n        rendered normally to get an image with the background color changed.\n        Conceptually, this works because it is tantamount to adding the expected\n        value that the ray is a background ray with color <code>color</code>, which\n        is just the probability that the ray doesn't terminate at any of the points\n        multiplied by <code>color</code>. After making this change to the volume\n        rendering equation, we can change the background color of the lego image as\n        shown below:\n    </p>\n    <video width={640} height={320} autoPlay=\"\" muted=\"\" loop=\"\">\n        <source src=\"images/nerfs/lego_color.mp4\" type=\"video/mp4\" />\n    </video>\n</>\n\n    )\n}\n\nexport default NERF;"],"mappings":"AAAA,MAAO,CAAAA,KAAK,KAAM,OAAO,CACzB,MAAO,oBAAoB,CAAC,OAAAC,GAAA,IAAAC,IAAA,gCAAAC,IAAA,IAAAC,KAAA,gCAAAC,QAAA,IAAAC,SAAA,yBAE5B,QAAS,CAAAC,IAAIA,CAAA,CAAG,CACZ,mBACAH,KAAA,CAAAE,SAAA,EAAAE,QAAA,eACAN,IAAA,OAAAM,QAAA,CAAI,wBAAsB,CAAI,CAAC,cAC/BN,IAAA,OAAIO,KAAK,CAAE,CAAEC,SAAS,CAAE,QAAS,CAAE,CAAAF,QAAA,CAAC,qBAAmB,CAAI,CAAC,cAC5DN,IAAA,OAAAM,QAAA,CAAI,YAAU,CAAI,CAAC,cACnBJ,KAAA,MAAAI,QAAA,EAAG,4HAEgD,CAAC,GAAG,CAAC,qBAAS,CAAC,GAAG,CAAC,QAAC,CAAC,GAAG,CAAC,cACjE,CAAC,GAAG,CAAC,mCAAiC,cAAAN,IAAA,MAAAM,QAAA,CAAG,OAAK,CAAG,CAAC,oEACjB,cAAAN,IAAA,MAAAM,QAAA,CAAG,OAAK,CAAG,CAAC,gBAAa,cAAAN,IAAA,MAAAM,QAAA,CAAG,QAAC,CAAG,CAAC,CAAC,GAAG,CAAC,wQAKlF,EAAG,CAAC,cACJN,IAAA,OAAAM,QAAA,CAAI,uDAAqD,CAAI,CAAC,cAC9DJ,KAAA,MAAAI,QAAA,EAAG,0DACyD,CAAC,GAAG,cAC5DJ,KAAA,MAAAI,QAAA,EAAG,KACA,CAAC,GAAG,CAAC,KAAG,CAAC,GAAG,CAAC,UAAG,CAAC,GAAG,CAAC,OAAK,CAAC,GAAG,EAC9B,CAAC,CAAC,GAAG,CAAC,k2BAY8D,CAAC,GAAG,EAC5E,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,4BAA4B,CAAAL,QAAA,cACpDN,IAAA,QAAKY,GAAG,CAAC,4BAA4B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACxD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,kCAAgC,CAAK,CAAC,EAC3D,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,MAAAM,QAAA,CAAG,oaAOH,CAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,2BAA2B,CAAAL,QAAA,cACnDN,IAAA,QAAKY,GAAG,CAAC,2BAA2B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACvD,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,0BAA0B,CAAAL,QAAA,cAClDN,IAAA,QAAKY,GAAG,CAAC,0BAA0B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACtD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,sCAAoC,CAAK,CAAC,EAC/D,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,MAAAM,QAAA,CAAG,gWAMH,CAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,4BAA4B,CAAAL,QAAA,cACpDN,IAAA,QAAKY,GAAG,CAAC,4BAA4B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACxD,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,2BAA2B,CAAAL,QAAA,cACnDN,IAAA,QAAKY,GAAG,CAAC,2BAA2B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACvD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,sCAAoC,CAAK,CAAC,EAC/D,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,MAAAM,QAAA,CAAG,ieAQH,CAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,4BAA4B,CAAAL,QAAA,cACpDN,IAAA,QAAKY,GAAG,CAAC,4BAA4B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACxD,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,2BAA2B,CAAAL,QAAA,cACnDN,IAAA,QAAKY,GAAG,CAAC,2BAA2B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACvD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,sCAAoC,CAAK,CAAC,EAC/D,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,MAAAM,QAAA,CAAG,iTAMH,CAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,+BAA+B,CAAAL,QAAA,cACvDN,IAAA,QAAKY,GAAG,CAAC,+BAA+B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CAC3D,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,8BAA8B,CAAAL,QAAA,cACtDN,IAAA,QAAKY,GAAG,CAAC,8BAA8B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CAC1D,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,sCAAoC,CAAK,CAAC,EAC/D,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,OAAAM,QAAA,CAAI,6DAA2D,CAAI,CAAC,cACpEN,IAAA,MAAAM,QAAA,CAAG,0NAIH,CAAG,CAAC,cACJN,IAAA,OAAAM,QAAA,CAAI,0BAAwB,CAAI,CAAC,cACjCJ,KAAA,MAAAI,QAAA,EAAG,6kBAQuE,CAAC,GAAG,cAC1EN,IAAA,MAAAM,QAAA,CAAG,GAAC,CAAG,CAAC,uUAIyB,cAAAN,IAAA,MAAAM,QAAA,CAAG,OAAK,CAAG,CAAC,+FACiB,CAAC,GAAG,cAClEJ,KAAA,MAAAI,QAAA,EAAG,GACF,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,EACV,CAAC,CAAC,GAAG,CAAC,GACR,CAAC,GAAG,cACLJ,KAAA,MAAAI,QAAA,EAAG,IACD,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,KAAE,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,KAAE,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,IAC1C,EAAG,CAAC,4BACqB,CAAC,GAAG,cAC7BJ,KAAA,MAAAI,QAAA,EAAG,GACF,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,EACV,CAAC,CAAC,GAAG,CAAC,MACL,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,KAAE,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,UAAO,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,KAAE,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,KAC/D,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,WAAQ,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,sMAGpC,EAAG,CAAC,cACJN,IAAA,OAAAM,QAAA,CAAI,UAAQ,CAAI,CAAC,cACjBJ,KAAA,MAAAI,QAAA,EAAG,wGAE4B,cAAAN,IAAA,SAAAM,QAAA,CAAM,aAAW,CAAM,CAAC,ocAMG,CAAC,GAAG,cAC1DN,IAAA,SAAAM,QAAA,CAAM,mBAAiB,CAAM,CAAC,wLAE+B,CAAC,GAAG,cACjEN,IAAA,SAAAM,QAAA,CAAM,6BAA2B,CAAM,CAAC,4CAC7B,CAAC,GAAG,cACfJ,KAAA,SAAAI,QAAA,EAAM,OACD,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,OAAI,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,OACjC,EAAM,CAAC,CAAC,GAAG,CAAC,mHAEyB,cAAAN,IAAA,SAAAM,QAAA,CAAM,GAAC,CAAM,CAAC,sHAEvC,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,SAAM,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,oBAAiB,CAAC,GAAG,cAC/DN,IAAA,SAAAM,QAAA,CAAM,gBAAc,CAAM,CAAC,+BAA4B,CAAC,GAAG,cAC3DN,IAAA,SAAAM,QAAA,CAAM,mBAAiB,CAAM,CAAC,EAC/B,CAAC,cACJN,IAAA,OAAAM,QAAA,CAAI,YAAU,CAAI,CAAC,cACnBJ,KAAA,MAAAI,QAAA,EAAG,wKAGkB,cAAAN,IAAA,MAAAM,QAAA,CAAG,GAAC,CAAG,CAAC,mDAAgD,CAAC,GAAG,cAC7EN,IAAA,SAAAM,QAAA,CAAM,aAAW,CAAM,CAAC,gBAAa,cAAAN,IAAA,SAAAM,QAAA,CAAM,SAAO,CAAM,CAAC,0FACU,CAAC,GAAG,cACvEN,IAAA,SAAAM,QAAA,CAAM,gBAAc,CAAM,CAAC,+GACoC,CAAC,GAAG,cACnEJ,KAAA,MAAAI,QAAA,EAAG,GACF,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,EACV,CAAC,CAAC,GAAG,CAAC,qCAC0B,cAAAN,IAAA,SAAAM,QAAA,CAAM,WAAS,CAAM,CAAC,8IAEN,CAAC,GAAG,EACxD,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,wBAAwB,CAAAL,QAAA,cAChDN,IAAA,QAAKY,GAAG,CAAC,wBAAwB,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACpD,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,OAAAM,QAAA,CAAI,mBAAiB,CAAI,CAAC,cAC1BJ,KAAA,MAAAI,QAAA,EAAG,wcAOU,cAAAN,IAAA,SAAAM,QAAA,CAAM,mBAAiB,CAAM,CAAC,yVAK/B,cAAAN,IAAA,MAAAM,QAAA,CAAG,GAAC,CAAG,CAAC,+CAA4C,CAAC,GAAG,cAChEJ,KAAA,MAAAI,QAAA,EAAG,GACF,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,EACV,CAAC,CAAC,GAAG,CAAC,2BACgB,CAAC,GAAG,EAC9B,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,4BAA4B,CAAAL,QAAA,cACpDN,IAAA,QAAKY,GAAG,CAAC,4BAA4B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACxD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,gCAA8B,CAAK,CAAC,EACzD,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,OAAAM,QAAA,CAAI,kBAAgB,CAAI,CAAC,cACzBJ,KAAA,MAAAI,QAAA,EAAG,6YAM+B,cAAAN,IAAA,SAAAM,QAAA,CAAM,cAAY,CAAM,CAAC,aAAU,CAAC,GAAG,cACrEN,IAAA,SAAAM,QAAA,CAAM,OAAK,CAAM,CAAC,uGACuB,cAAAN,IAAA,SAAAM,QAAA,CAAM,gBAAc,CAAM,CAAC,CAAC,GAAG,CAAC,kDAE7E,EAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,6BAA6B,CAAAL,QAAA,cACrDN,IAAA,QAAKY,GAAG,CAAC,6BAA6B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACzD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,2BAAyB,CAAK,CAAC,EACpD,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,OAAAM,QAAA,CAAI,SAAO,CAAI,CAAC,cAChBN,IAAA,MAAAM,QAAA,CAAG,6LAIH,CAAG,CAAC,cACJN,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAAAH,QAAA,cAC3BN,IAAA,QAAKS,SAAS,CAAC,SAAS,CAAAH,QAAA,cACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,4BAA4B,CAAAL,QAAA,cACpDN,IAAA,QAAKY,GAAG,CAAC,4BAA4B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACxD,CAAC,CACH,CAAC,CACD,CAAC,CACL,CAAC,cACNb,IAAA,QAAKS,SAAS,CAAC,WAAW,CAAAH,QAAA,cACtBN,IAAA,QAAKS,SAAS,CAAC,YAAY,CAACF,KAAK,CAAE,CAAEO,KAAK,CAAE,MAAO,CAAE,CAAAR,QAAA,cACrDJ,KAAA,QAAKO,SAAS,CAAC,SAAS,CAAAH,QAAA,eACpBN,IAAA,MAAGU,MAAM,CAAC,QAAQ,CAACC,IAAI,CAAC,2BAA2B,CAAAL,QAAA,cACnDN,IAAA,QAAKY,GAAG,CAAC,2BAA2B,CAACC,GAAG,CAAC,cAAc,CAAE,CAAC,CACvD,CAAC,cACJb,IAAA,QAAKS,SAAS,CAAC,MAAM,CAAAH,QAAA,CAAC,kDAEtB,CAAK,CAAC,EACL,CAAC,CACD,CAAC,CACL,CAAC,cACNN,IAAA,MAAAM,QAAA,CAAG,0LAIH,CAAG,CAAC,cACJN,IAAA,UAAOc,KAAK,CAAE,GAAI,CAACC,MAAM,CAAE,GAAI,CAACC,QAAQ,CAAC,EAAE,CAACC,KAAK,CAAC,EAAE,CAACC,IAAI,CAAC,EAAE,CAAAZ,QAAA,cACxDN,IAAA,WAAQY,GAAG,CAAC,uBAAuB,CAACO,IAAI,CAAC,WAAW,CAAE,CAAC,CACpD,CAAC,cACRnB,IAAA,OAAAM,QAAA,CAAI,kBAAoB,CAAI,CAAC,cAC7BN,IAAA,OAAAM,QAAA,CAAI,sBAAoB,CAAI,CAAC,cAC7BJ,KAAA,MAAAI,QAAA,EAAG,mOAGyE,CAAC,GAAG,cAC5EN,IAAA,SAAAM,QAAA,CAAM,kBAAgB,CAAM,CAAC,mGAEjC,EAAG,CAAC,cACJN,IAAA,UAAOc,KAAK,CAAE,GAAI,CAACC,MAAM,CAAE,GAAI,CAACC,QAAQ,CAAC,EAAE,CAACC,KAAK,CAAC,EAAE,CAACC,IAAI,CAAC,EAAE,CAAAZ,QAAA,cACxDN,IAAA,WAAQY,GAAG,CAAC,6BAA6B,CAACO,IAAI,CAAC,WAAW,CAAE,CAAC,CAC1D,CAAC,cACRnB,IAAA,OAAAM,QAAA,CAAI,sBAAoB,CAAI,CAAC,cAC7BJ,KAAA,MAAAI,QAAA,EAAG,iGAEqB,CAAC,GAAG,cACxBJ,KAAA,SAAAI,QAAA,EAAM,WACG,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,iBAAS,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,SAAC,cAAAN,IAAA,QAAAM,QAAA,CAAK,GAAC,CAAK,CAAC,YACvD,EAAM,CAAC,CAAC,GAAG,CAAC,2VAKsC,cAAAN,IAAA,SAAAM,QAAA,CAAM,OAAK,CAAM,CAAC,qGAEtD,cAAAN,IAAA,SAAAM,QAAA,CAAM,OAAK,CAAM,CAAC,oIAGpC,EAAG,CAAC,cACJN,IAAA,UAAOc,KAAK,CAAE,GAAI,CAACC,MAAM,CAAE,GAAI,CAACC,QAAQ,CAAC,EAAE,CAACC,KAAK,CAAC,EAAE,CAACC,IAAI,CAAC,EAAE,CAAAZ,QAAA,cACxDN,IAAA,WAAQY,GAAG,CAAC,6BAA6B,CAACO,IAAI,CAAC,WAAW,CAAE,CAAC,CAC1D,CAAC,EACV,CAAC,CAGH,CAEA,cAAe,CAAAd,IAAI","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}